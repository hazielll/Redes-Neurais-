{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando uma rede neural\n",
    "=========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de uma longa jornada, finalmente chegamos ao *season finale* da nossa saga para construir uma rede neural artificial em Python puro. Agora que já conseguimos criar uma rede neural, o próximo passo é treinar essa rede.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar uma rede neural artificial tipo Multilayer Perceptron usando Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Valor:\n",
    "    def __init__(self, data, progenitor=(), operador_mae=\"\", rotulo=\"\"):\n",
    "        self.data = data\n",
    "        self.progenitor = progenitor\n",
    "        self.operador_mae = operador_mae\n",
    "        self.rotulo = rotulo\n",
    "        self.grad = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Valor(data={self.data})\"\n",
    "\n",
    "    def __add__(self, outro_valor):\n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "        data = self.data + outro_valor.data\n",
    "        progenitor = (self, outro_valor)\n",
    "        operador_mae = \"+\"\n",
    "        saida = Valor(data, progenitor, operador_mae)\n",
    "\n",
    "        def propagar_adicao():\n",
    "            self.grad += saida.grad * 1\n",
    "            outro_valor.grad += saida.grad * 1\n",
    "\n",
    "        saida.propagar = propagar_adicao\n",
    "\n",
    "        return saida\n",
    "    \n",
    "    def __radd__(self, outro_valor): # outro_valor + self\n",
    "        return self + outro_valor\n",
    "\n",
    "    def __mul__(self, outro_valor):\n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "        data = self.data * outro_valor.data\n",
    "        progenitor = (self, outro_valor)\n",
    "        operador_mae = \"*\"\n",
    "        saida = Valor(data, progenitor, operador_mae)\n",
    "\n",
    "        def propagar_multiplicacao():\n",
    "            self.grad += saida.grad * outro_valor.data\n",
    "            outro_valor.grad += saida.grad * self.data\n",
    "\n",
    "        saida.propagar = propagar_multiplicacao\n",
    "\n",
    "        return saida\n",
    "    \n",
    "    def __rmul__(self, outro_valor): # outro_valor * self\n",
    "        return self * outro_valor\n",
    "    \n",
    "    def __pow__(self, expoente):  # self ** expoente\n",
    "        \n",
    "        assert isinstance(expoente, (int, float))\n",
    "\n",
    "        data = self.data ** expoente\n",
    "        progenitor = (self, )\n",
    "        operador_mae = f\"**{expoente}\"\n",
    "        saida = Valor(data, progenitor, operador_mae)\n",
    "\n",
    "        def propagar_exponenciacao():\n",
    "            self.grad += saida.grad * expoente * (self.data ** (expoente - 1))\n",
    "\n",
    "        saida.propagar = propagar_exponenciacao\n",
    "\n",
    "        return saida\n",
    "    \n",
    "    def __truediv__(self, outro_valor): # self / outro_valor\n",
    "        return self * outro_valor ** (-1)\n",
    "    \n",
    "    def __neg__(self):  # - self\n",
    "        return self * (-1)\n",
    "    \n",
    "    def __sub__(self, outro_valor):  # self - outro_valor\n",
    "        return self + (-outro_valor)\n",
    "    \n",
    "    def __rsub__(self, outro_valor):  # outro_valor - self\n",
    "        return self * (-1) + outro_valor\n",
    "    \n",
    "    def exp(self):\n",
    "\n",
    "        data = math.exp(self.data)\n",
    "        progenitor = (self, )\n",
    "        operador_mae = \"exp\"\n",
    "        saida = Valor(data, progenitor, operador_mae)\n",
    "\n",
    "        def propagar_exp():\n",
    "            self.grad += saida.grad * data\n",
    "\n",
    "        saida.propagar = propagar_exp\n",
    "\n",
    "        return saida\n",
    "    \n",
    "    def sig(self): \n",
    "        return self.exp() / (self.exp() + 1)\n",
    "\n",
    "    def propagar(self):\n",
    "        pass\n",
    "\n",
    "    def propagar_tudo(self):\n",
    "        ordem_topologica = []\n",
    "        visitados = set()\n",
    "\n",
    "        def constroi_ordem_topologica(v):\n",
    "            if v not in visitados:\n",
    "                visitados.add(v)\n",
    "                for progenitor in v.progenitor:\n",
    "                    constroi_ordem_topologica(progenitor)\n",
    "                ordem_topologica.append(v)\n",
    "\n",
    "        constroi_ordem_topologica(self)\n",
    "\n",
    "        self.grad = 1  # o gradiente do vértice folha deve ser 1\n",
    "\n",
    "        for v in reversed(ordem_topologica):\n",
    "            v.propagar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código e discussão\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo estão as classes que criamos no notebook anterior. Elas são as classes de base para criarmos nossa rede neural MLP. Ao longo deste notebook vamos fazer as últimas modificações nelas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuronio:\n",
    "    def __init__(self, num_dados_entrada):\n",
    "        pesos = []\n",
    "\n",
    "        for _ in range(num_dados_entrada):\n",
    "            peso = Valor(random.uniform(-1, 1))\n",
    "            pesos.append(peso)\n",
    "\n",
    "        self.pesos = pesos\n",
    "        self.vies = Valor(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Executa a computação interna do neurônio.\n",
    "\n",
    "        Args:\n",
    "          x: lista de valores de entrada (informação que chega ao neurônio).\n",
    "\n",
    "        Returns:\n",
    "          Informação que o neurônio propaga adiante. Trata-se da\n",
    "          aplicação da função de ativação à soma de `x` vezes os pesos do\n",
    "          neurônio adicionado ao viés.\n",
    "        \"\"\"\n",
    "        assert len(x) == len(self.pesos), \"Seu x tem tamanho errado.\"\n",
    "\n",
    "        soma = 0\n",
    "        for x_, p in zip(x, self.pesos):\n",
    "            soma = soma + x_ * p\n",
    "\n",
    "        soma = soma + self.vies\n",
    "        dado_de_saida = soma.sig()\n",
    "        return dado_de_saida\n",
    "    \n",
    "    def parametros(self):\n",
    "        return self.pesos + [self.vies]\n",
    "\n",
    "\n",
    "class Camada:\n",
    "    def __init__(self, num_dados_entrada, num_neuronios):\n",
    "        neuronios = []\n",
    "\n",
    "        for _ in range(num_neuronios):\n",
    "            neuronio = Neuronio(num_dados_entrada)\n",
    "            neuronios.append(neuronio)\n",
    "\n",
    "        self.neuronios = neuronios\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Executa a computação de cada neurônio da camada.\n",
    "\n",
    "        Args:\n",
    "          x: lista de valores de entrada (informação que chega aos neurônios).\n",
    "\n",
    "        Returns:\n",
    "          Informação que os neurônios da camada propagam adiante.\n",
    "        \"\"\"\n",
    "        saidas = []\n",
    "\n",
    "        for neuronio in self.neuronios:\n",
    "            informacao = neuronio(x)\n",
    "            saidas.append(informacao)\n",
    "\n",
    "        if len(saidas) == 1:\n",
    "            return saidas[0]\n",
    "        else:\n",
    "            return saidas\n",
    "        \n",
    "    def parametros(self):        \n",
    "        params = []        \n",
    "        for neuronio in self.neuronios:            \n",
    "            params.extend(neuronio.parametros())\n",
    "        return params            \n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_dados_entrada, num_neuronios_por_camada):\n",
    "\n",
    "        percurso = [num_dados_entrada] + num_neuronios_por_camada\n",
    "        camadas = []\n",
    "\n",
    "        for i in range(len(num_neuronios_por_camada)):\n",
    "            camada = Camada(percurso[i], percurso[i+1])\n",
    "            camadas.append(camada)\n",
    "\n",
    "        self.camadas = camadas\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Executa a computação de cada camada.\n",
    "\n",
    "        Como a rede MLP é uma rede feedforward, a informação que uma camada\n",
    "        recebe é a informação de saída da camada anterior. A única exceção é a\n",
    "        camada de entrada, onde a informação é fornecida pelo usuário\n",
    "\n",
    "        Args:\n",
    "          x: informação fornecida na camada de entrada.\n",
    "\n",
    "        Returns:\n",
    "          Informação recuperada na camada de saída.\n",
    "        \"\"\"\n",
    "        for camada in self.camadas:\n",
    "            x = camada(x)\n",
    "        return x\n",
    "    \n",
    "    def parametros(self):\n",
    "        params = []\n",
    "        for camada in self.camadas:\n",
    "            params.extend(camada.parametros())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A função de perda (*loss function*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fomos ao laboratório e sintetizamos 4 amostras de [complete com o que quiser]. Cada uma dessas amostras foi feita variando 3 parâmetros de processamento diferentes. Estas amostras e seus parâmetros de processamento estão representados na variável `x` abaixo.\n",
    "\n",
    "Levamos essas 4 amostras no equipamento [complete com o que quiser] e obtivemos uma resposta que está representada na variável `y_true` abaixo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "y_true = [1, 0, 0.2, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos treinar uma MLP que seja capaz de modelar o comportamento que observamos. Isto é, queremos uma rede neural que receba informação sobre os 3 parâmetros de processamento e que com isso seja capaz de prever qual o resultado que teríamos no equipamento que mencionamos no parágrafo anterior.\n",
    "\n",
    "Vamos, primeiramente, criar uma rede neural simples do tipo MLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 3  # são 3 parâmetros que descrevem cada amostra\n",
    "NUM_DADOS_DE_SAIDA = 1    # queremos apenas um valor de saída por amostra\n",
    "CAMADAS_OCULTAS = [3, 2]  # fique à vontade para alterar aqui\n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]\n",
    "\n",
    "minha_mlp = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com nossa rede criada, podemos realizar uma previsão! Mas muito provavelmente esta previsão será bastante subótima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0.2, 0.5]\n",
      "[Valor(data=0.3089741696982651), Valor(data=0.31657158326021506), Valor(data=0.3120011156011473), Valor(data=0.3108821884295717)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "y_pred = []\n",
    "\n",
    "for x_ in x:\n",
    "    valor_previsto = minha_mlp(x_)\n",
    "    y_pred.append(valor_previsto)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso resultado, assim como esperado, é bastante subótimo. Mas como quantificar isso? Seria bom se existisse uma forma de quantificar em um único número o quão boa está a previsão da minha rede.\n",
    "\n",
    "Uma forma de quantificar a qualidade da previsão é usando uma `função de perda`, mais conhecida como `loss function`. Aqui podemos, por exemplo, computar a soma dos erros quadráticos. Esta é uma função de perda muito usada em problemas de regressão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor(data=0.6260440620211709)\n"
     ]
    }
   ],
   "source": [
    "erros_quadrados = []\n",
    "\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    erro_quadratico = (yp - yt) ** 2\n",
    "    erros_quadrados.append(erro_quadratico)\n",
    "\n",
    "loss = sum(erros_quadrados)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável `loss` é uma medida da performance da rede neural que criamos. Essa variável é uma instância de `Valor`, logo podemos facilmente observar o grafo computacional desta métrica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plota_grafo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HAZIEL~1\\AppData\\Local\\Temp/ipykernel_16140/1868130149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagar_tudo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrafo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplota_grafo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgrafo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rede_neural\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgrafo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plota_grafo' is not defined"
     ]
    }
   ],
   "source": [
    "loss.propagar_tudo()\n",
    "grafo = plota_grafo(loss)\n",
    "grafo.render(\"rede_neural\", format=\"png\")\n",
    "grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede através da atualização dos parâmetros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos acima, o grafo computacional da nossa rede neural é bastante complexo! Cheio de vértices!\n",
    "\n",
    "Nosso objetivo é treinar a rede neural que criamos, sendo que para isso precisamos alterar os parâmetros internos da rede. O primeiro passo para fazer isso é alterar as nossas classes de forma que o código abaixo funcione.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Valor(data=0.14474677867167784),\n",
       " Valor(data=-0.42740964094034606),\n",
       " Valor(data=-0.3828776591909242),\n",
       " Valor(data=0.08969170467927734),\n",
       " Valor(data=0.05029964556279287),\n",
       " Valor(data=0.12538403057155123),\n",
       " Valor(data=0.1457220816182896),\n",
       " Valor(data=-0.16740138252949466),\n",
       " Valor(data=-0.125568827894476),\n",
       " Valor(data=0.3065202687146462),\n",
       " Valor(data=-0.5043999071852516),\n",
       " Valor(data=0.9404563310512),\n",
       " Valor(data=0.45134197667918086),\n",
       " Valor(data=-0.9746388264339698),\n",
       " Valor(data=0.7829235978448743),\n",
       " Valor(data=0.3326052883164423),\n",
       " Valor(data=-0.09122532539318695),\n",
       " Valor(data=-0.12203984696584791),\n",
       " Valor(data=0.3904841078490888),\n",
       " Valor(data=-0.20468933340137818),\n",
       " Valor(data=0.10944135968276703),\n",
       " Valor(data=-0.8781084652702196),\n",
       " Valor(data=-0.42872869226646215)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametros = minha_mlp.parametros()\n",
    "\n",
    "print(len(parametros))\n",
    "print()\n",
    "parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que conseguimos &ldquo;extrair&rdquo; os parâmetros da nossa MLP, podemos fazer o treino deles! Temos que pensar bem como alterar os parâmetros da maneira correta. Lembre-se que nosso desejo é reduzir a métrica computada pela função de perda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXA_DE_APRENDIZADO = 0.01\n",
    "\n",
    "for p in minha_mlp.parametros():\n",
    "    p.data = p.data - p.grad * TAXA_DE_APRENDIZADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que executamos um &ldquo;ciclo&rdquo; de treino, podemos observar o impacto disso!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor(data=0.618908695726716)\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for x_ in x:\n",
    "    valor_previsto = minha_mlp(x_)\n",
    "    y_pred.append(valor_previsto)\n",
    "\n",
    "erros_quadrados = []\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    erro_quadratico = (yp - yt) ** 2\n",
    "    erros_quadrados.append(erro_quadratico)\n",
    "\n",
    "loss = sum(erros_quadrados)\n",
    "loss.propagar_tudo()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A descida do gradiente\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas um &ldquo;ciclo&rdquo; de treino costuma não ser suficiente para treinarmos uma rede neural, precisamos de mais! Antes de seguir em frente, vamos definir um termo: chamamos de `época` toda vez que nossa rede neural propaga *todo* nosso dataset. É comum treinarmos redes neurais por dezenas, centenas e até milhares de épocas!\n",
    "\n",
    "Vamos programar um treino completo da rede neural!\n",
    "\n",
    "Um detalhe: sempre temos que zerar os gradientes antes de fazer o backpropagation. Isso é necessário pois cada vez que alteramos os parâmetros estaremos em outra posição da curva de perda e os gradientes antigos já não são mais válidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 3  # são 3 parâmetros que descrevem cada amostra\n",
    "NUM_DADOS_DE_SAIDA = 1    # queremos apenas um valor de saída por amostra\n",
    "CAMADAS_OCULTAS = [3, 2]  # fique à vontade para alterar aqui\n",
    "\n",
    "x = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "y_true = [1, 0, 0.2, 0.5]\n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]\n",
    "minha_mlp = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.937049395404859\n",
      "1 0.9337973542527139\n",
      "2 0.9305540477843443\n",
      "3 0.9273197396285877\n",
      "4 0.9240946913182901\n",
      "5 0.9208791621819772\n",
      "6 0.9176734092369213\n",
      "7 0.9144776870836978\n",
      "8 0.9112922478023403\n",
      "9 0.9081173408501902\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 10\n",
    "TAXA_DE_APRENDIZADO = 0.01\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for x_ in x:\n",
    "        previsao = minha_mlp(x_)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp.parametros():\n",
    "        p.grad = 0\n",
    "    \n",
    "    # loss\n",
    "    loss = []\n",
    "    for yp, yt in zip(y_pred, y_true):\n",
    "        loss.append((yp - yt)**2)\n",
    "    loss = sum(loss)        \n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    for p in minha_mlp.parametros():\n",
    "        p.data = p.data - p.grad * TAXA_DE_APRENDIZADO\n",
    "\n",
    "    # mostra resultado\n",
    "    print(epoca, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treino, podemos checar se nossa rede é capaz de prever os dados que coletamos com boa performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0.2, 0.5]\n",
      "[Valor(data=0.7380828185480234), Valor(data=0.7166093215754444), Valor(data=0.7210031308816975), Valor(data=0.7335456031551277)]\n"
     ]
    }
   ],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse processo de atualizar os parâmetros da rede neural observando os gradientes locais é chamado de `descida do gradiente` (ou apenas `método do gradiente`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "Neste aula foi finalizada, de fato, a rede neural que estava sendo construída - sem o uso de bibliotecas, por enquanto. Por conta disso, fomos apresentados à função de perda/loss, o que mostra se uma previsão realizada é boa ou não, de maneira similar ao fitness dos algoritmos genéticos. Para aplicar a função de perda, é necessário realizar o cálculo dos quadrados mínimos. <br>\n",
    "Com essa função para avaliar o desempenho, o próximo passo foi definir quais seriam os parâmetros utilizados, sendo eles: os pesos e os viéses. Para isso, foi necessário complementar a as classes criadas com o metodo de parâmetros, responsável por checkar os parâmetros em questão. <br>\n",
    "Após isso, nos foi apresentado o conceito de épocas, as quais consistem em iterações nas quais todos os exemplos de treinamento são apresentados à rede neural, permitindo que os pesos sejam atualizados com base nos erros de previsão, visando melhorar a precisão da rede. <br>\n",
    "Com nossas ferramentas quase completas, faltam apenas aplicá-las à nossa rede para que seu treinamento possa ser realizado. Dessa forma, foram revisitados brevemente os conceitos de `forward pass/forward propagations`, gradientes, a nova função de perda, a backpropagation e, finalmente, os parâmetros foram atualizados. Vale ressaltar que esses parâmetros estão ligados à taxa de aprendizado introduzida. <br>\n",
    "Dessa forma, concluímos nossa primeira rede neural, a qual, por ser básica, auxiliou na nossa melhor compreensão do `core` do funcionamento das redes neurais no geral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
